# 梯度下降
通过迭代计算局部最小值点的方法。  
机器学习中常见模型，已知损失函数f(w)，其中w是权重矩阵，目标是求w，使得f(w)最小。  
迭代规则 $w_{n} = w_{n-1} - \eta f'(w)$  
迭代过程演示:  
以 $f(x)=x^2+2x+1$ 为例， $f'(x)=2x+2$ ，目标求 $x$ 使得 $f(x)$ 达到最小值，设定开始叠代初始值0.  
$x_0=0, \eta=0.05, $  
$f'(0)=2, x_1=0-0.05 \times 2=-0.1 $  
$f'(-0.1)=1.8, x_2=-0.1-1.8 \times 0.05=-0.19 $  
$x_3=-0.271, x_4=-0.3439, ..., x_{100}=-0.9999734386011123 $  
![sgd](https://nicky918.github.io/images/posts/2017/SGD-1.png)  
可梯度求解的条件 L-Lipschitz continuous  
存在常数 $L$，对于作用域 $D$ 任意 $x,y$ ， 总是满足 $||\nabla x - \nabla y|| \le L||x-y||$  
多维举例:  
$f(x_0,x_1)=(x_0-4)^2+(x_1-2)^2, f'(x_0)=2(x_0-4),f'(x_1)=2(x_1-2) $  
$x_0=0, x_1=0, \eta=0.05 $  
对$x_0$的迭代公式 $x_n=x_{n-1}-0.05*2(x_{n-1}-4)$ 对$x_1$的迭代公式 



# Softmax
# Cross Entropy Loss Function(交叉熵损失函数)
# Word2Vec
